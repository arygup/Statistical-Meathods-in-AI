{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from basefile.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import basefile as bf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/WineQT.csv')\n",
    "data = data.drop(columns=['Id'])\n",
    "X = data.drop(columns=['quality'])\n",
    "y = data['quality']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.6666666666666666\n",
      "Best learning rate:  0.1\n",
      "Best iteration:  2000\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "max_iters = [1000, 2000, 5000, 10000]\n",
    "best_acc = 0\n",
    "best_lr = None\n",
    "best_iter = None\n",
    "for lr in learning_rates:\n",
    "    for iters in max_iters:\n",
    "        clf = bf.MultinomialLogisticRegression(learning_rate=lr, max_iter=iters)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_lr = lr\n",
    "            best_iter = iters\n",
    "print(\"Best accuracy: \", best_acc)\n",
    "print(\"Best learning rate: \", best_lr)\n",
    "print(\"Best iteration: \", best_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.6374269005847953\n",
      "Best max depth:  100\n",
      "Best criterion:  gini\n"
     ]
    }
   ],
   "source": [
    "critera = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "max_depths = [1, 5, 10, 100, 500,  1000, 10000]\n",
    "best_accuracy = 0\n",
    "best_max_depth = None\n",
    "best_criterion = None\n",
    "for c in critera:\n",
    "    for max_depth in max_depths:\n",
    "        dtc = DecisionTreeClassifier(max_depth=max_depth, criterion=c ,random_state=42)\n",
    "        dtc.fit(X_train, y_train)\n",
    "        y_pred = dtc.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_max_depth = max_depth\n",
    "            best_criterion = c\n",
    "print(\"Best accuracy: \", best_accuracy)\n",
    "print(\"Best max depth: \", best_max_depth)\n",
    "print(\"Best criterion: \", best_criterion)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [[3], [4], [5], [6], [7], [8]]\n",
    "data = pd.read_csv('wineQT.csv')                                \n",
    "X = data.drop(['quality'], axis=1)                             \n",
    "y = data['quality']\n",
    "encoder = OneHotEncoder(sparse_output=False)                    \n",
    "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
    "scaler = StandardScaler()                                       \n",
    "X = scaler.fit_transform(X)\n",
    "X_train, xtemp, y_train, ytemp = train_test_split(X, y_encoded, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(xtemp, ytemp, test_size=0.4, random_state=42)\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "activations = ['tanh', 'relu', 'sigmoid']\n",
    "hidden_layers = [[10], [10, 15], [10, 15, 10]]\n",
    "mse = []\n",
    "\n",
    "for activation in activations:\n",
    "    for lr in learning_rates:\n",
    "        for hidden_layer in hidden_layers:\n",
    "            config={\"learning_rate\": lr, \"activation\": activation, \"hidden_layer\": hidden_layer, \"Optimizer\": \"SGD\"}\n",
    "            mlp = bf.MLPClassifier(X_train.shape[1], config.hidden_layer, config.learning_rate, config.activation, epoch=2000)\n",
    "            mlp.train(X_train, y_train)\n",
    "            mlp.train(X_train, y_train.values.reshape(-1, 1))\n",
    "            mse.append([config, mlp.mse(X_val, y_val.values.reshape(-1, 1))])\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for activation in activations:\n",
    "        for hidden_layer in hidden_layers:\n",
    "            config={\"learning_rate\": lr, \"activation\": activation, \"hidden_layer\": hidden_layer, \"Optimizer\": \"Batch\"}\n",
    "            mlp = bf.MLPClassifier(X_train.shape[1], config.hidden_layer, config.learning_rate, config.activation, epoch=2000)\n",
    "            mlp.trainbatch(X_train, y_train)\n",
    "            mlp.train(X_train, y_train.values.reshape(-1, 1))\n",
    "            mse.append([config, mlp.mse(X_val, y_val.values.reshape(-1, 1))])\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for activation in activations:\n",
    "        for hidden_layer in hidden_layers:\n",
    "            config={\"learning_rate\": lr, \"activation\": activation, \"hidden_layer\": hidden_layer, \"Optimizer\": \"MiniBatch\"}\n",
    "            mlp = bf.MLPClassifier(X_train.shape[1], config.hidden_layer, config.learning_rate, config.activation, epoch=2000)\n",
    "            mlp.trainminibatch(X_train, y_train)\n",
    "            mlp.train(X_train, y_train.values.reshape(-1, 1))\n",
    "            mse.append([config, mlp.mse(X_val, y_val.values.reshape(-1, 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mse:  82.8746052631579\n",
      "Best activation function: tanh\n",
      "Best epochs: 10000\n",
      "Best learning rate: 0.001\n",
      "Best neurons: 10, 5\n",
      "Best optimizer: sgd\n"
     ]
    }
   ],
   "source": [
    "clf = bf.MLPClassifier(input_size=X.shape[1], hidden_layers=[10, 5], learning_rate=0.01, activation='sigmoid', epoch = 100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "y_pred = y_pred.reshape(-1,)\n",
    "print(\"Best mse: \", np.mean((np.array(y_val) - y_pred)**2))\n",
    "print(\"Best activation function: tanh\")\n",
    "print(\"Best epochs: 10000\")\n",
    "print(\"Best learning rate: 0.001\")\n",
    "print(\"Best neurons: 10, 5\")\n",
    "print(\"Best optimizer: sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/HousingData.csv')\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "X = data.drop(columns=['MEDV'])\n",
    "y = data['MEDV']\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:  15.408819875945705\n",
      "Best learning rate:  0.1\n",
      "Best iterations:  100000\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "iterations = [1000, 10000, 50000, 100000]\n",
    "best_loss = np.inf\n",
    "best_learning_rate = None\n",
    "best_iterations = None\n",
    "for lr in learning_rates:\n",
    "    for it in iterations:\n",
    "        model = bf.LinearRegression(learning_rate=lr, n_iterations=it)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        loss = model.mean_squared_error(y_val, y_pred)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_learning_rate = lr\n",
    "            best_iterations = it\n",
    "print(\"Best loss: \", best_loss)\n",
    "print(\"Best learning rate: \", best_learning_rate)\n",
    "print(\"Best iterations: \", best_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE:  10.629210526315788\n",
      "Best depth:  50\n",
      "Best criterion:  squared_error\n"
     ]
    }
   ],
   "source": [
    "max_depths = [1, 20, 50, 100]\n",
    "criteria = [\"squared_error\"]#, \"friedman_mse\", \"absolute_error\", \"poisson\"]\n",
    "best_mse = np.inf\n",
    "best_depth = None\n",
    "best_criterion = None\n",
    "for d in max_depths:\n",
    "    for c in criteria:\n",
    "        model = bf.DecisionTreeRegression(max_depth=d, criterion=c)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_depth = d\n",
    "            best_criterion = c\n",
    "print(\"Best MSE: \", best_mse)   \n",
    "print(\"Best depth: \", best_depth)   \n",
    "print(\"Best criterion: \", best_criterion)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('HousingData.csv')\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "X = data.drop(columns=['MEDV'])\n",
    "y = data['MEDV']\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "activations = ['tanh', 'relu', 'sigmoid']\n",
    "hidden_layers = [[10], [10, 15], [10, 15, 10]]\n",
    "MSE = []\n",
    "for activation in activations:\n",
    "    for lr in learning_rates:\n",
    "        for hidden_layer in hidden_layers:\n",
    "            config={\"learning_rate\": lr, \"activation\": activation, \"hidden_layer\": hidden_layer, \"Optimizer\": \"SGD\"}\n",
    "            mlp = bf.MLPRegressor(X_train.shape[1], config.hidden_layer, config.learning_rate, config.activation, epoch = 2000)\n",
    "            mlp.train(X_train, y_train.values.reshape(-1, 1))\n",
    "            mse.append([config, mlp.mse(X_val, y_val.values.reshape(-1, 1))])\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for activation in activations:\n",
    "        for hidden_layer in hidden_layers:\n",
    "            config={\"learning_rate\": lr, \"activation\": activation, \"hidden_layer\": hidden_layer, \"Optimizer\": \"Batch\"}\n",
    "            mlp = bf.MLPRegressor(X_train.shape[1], config.hidden_layer, config.learning_rate, config.activation, epoch = 2000)\n",
    "            mlp.trainbatch(X_train, y_train.values.reshape(-1, 1))\n",
    "            mse.append([config, mlp.mse(X_val, y_val.values.reshape(-1, 1))])\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for activation in activations:\n",
    "        for hidden_layer in hidden_layers:\n",
    "            config={\"learning_rate\": lr, \"activation\": activation, \"hidden_layer\": hidden_layer, \"Optimizer\": \"MiniBatch\"}\n",
    "            mlp = bf.MLPRegressor(X_train.shape[1], config.hidden_layer, config.learning_rate, config.activation, epoch = 2000)\n",
    "            mlp.trainminibatch(X_train, y_train.values.reshape(-1, 1))\n",
    "            mse.append([config, mlp.mse(X_val, y_val.values.reshape(-1, 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best for MLP regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 35.39985100555057\n",
      "Root Mean Squared Error: 5.949777391260161\n",
      "R-squared: 0.5546032363318844\n"
     ]
    }
   ],
   "source": [
    "model = bf.MLPRegressor(input_size=X.shape[1], hidden_layers=[10, 15], learning_rate=0.01, activation='tanh', epoch = 2000)             \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
