{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegression:\n",
    "    def __init__(self, criterion='squared_error', splitter='best', max_depth=None):\n",
    "        self.model = DecisionTreeRegressor(criterion=criterion, splitter=splitter, max_depth=max_depth)\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        self.confidence = np.mean((y - self.predict(X)) ** 2)\n",
    "    def mean_squared_error(self, y_pred, y_true):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.weights = np.random.randn(n_features)\n",
    "        self.bias = 0\n",
    "        for i in range(self.n_iterations):\n",
    "            model = np.dot(X_train, self.weights) + self.bias\n",
    "            dw = (1/n_samples) * np.dot(X_train.T, (model - y_train))\n",
    "            db = (1/n_samples) * np.sum(model - y_train)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "        self.confidence = np.mean((y_train - self.predict(X_train)) ** 2)\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    def mean_squared_error(self, y_pred, y_true):\n",
    "        return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionCustom: \n",
    "    def __init__(self, learning_rate=0.0001, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]  # Add a column of ones for bias\n",
    "        y_train = y_train.values.reshape(-1, 1)\n",
    "        n_samples, n_features = X_train.shape\n",
    "\n",
    "        self.weights = np.ones((n_features, 1))\n",
    "        self.bias = 1\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            predictions = np.dot(X_train, self.weights) + self.bias\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1/n_samples) * np.dot(X_train.T, (predictions - y_train))\n",
    "            db = (1/n_samples) * np.sum(predictions - y_train)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            # break if any value < e-5\n",
    "            if np.any(self.weights < 1e-10):\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X = np.c_[np.ones((X.shape[0], 1)), X]  # Add a column of ones for bias\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train and y_train are your training data\n",
    "# model = LinearRegressionCustom()\n",
    "# model.fit(X_train, y_train)\n",
    "# predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = []\n",
    "        self.classes = None\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def _compute_cost(self, X, y, weights):\n",
    "        m = X.shape[0]\n",
    "        h = self._sigmoid(X @ weights)\n",
    "        cost = -1/m * np.sum(y * np.log(h + 1e-5) + (1 - y) * np.log(1 - h + 1e-5))\n",
    "        return cost\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        X = np.insert(X, 0, 1, axis=1)  \n",
    "        for c in self.classes:\n",
    "            y_c = (y == c).astype(int)\n",
    "            weights_c = np.zeros(X.shape[1])\n",
    "            for _ in range(self.max_iter):\n",
    "                m = X.shape[0]\n",
    "                h = self._sigmoid(X @ weights_c)\n",
    "                gradient = np.dot(X.T, (h - y_c)) / m\n",
    "                weights_c -= self.learning_rate * gradient\n",
    "            self.weights.append(weights_c)\n",
    "    def predict_proba(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)  \n",
    "        probs = np.array([self._sigmoid(np.dot(X, w)) for w in self.weights]).T\n",
    "        return probs / np.sum(probs, axis=1, keepdims=True)\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.array([self.classes[np.argmax(p)] for p in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor: #A3\n",
    "    def __init__(self, input_size, hidden_layers, learning_rate, activation, epoch):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = 1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.neurons_layer = [input_size] + hidden_layers + [1]\n",
    "        std_deviation = np.sqrt(2.0 / input_size)\n",
    "        self.weights = [np.random.randn(self.neurons_layer[i], self.neurons_layer[i + 1])*std_deviation for i in range(len(self.neurons_layer) - 1)]\n",
    "        self.biases = [np.zeros((1, neurons)) for neurons in self.neurons_layer[1:]]\n",
    "        self.activation = activation\n",
    "        self.istrain = 0\n",
    "        self.epoch = epoch\n",
    "        self.confidence = 1\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.trainminibatch(X_train, y_train.values.reshape(-1, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif(self.activation == 'tanh'):\n",
    "            exp_x = np.exp(2 * x)\n",
    "            tanh_x = (exp_x - 1) / (exp_x + 1)\n",
    "            return tanh_x\n",
    "        elif(self.activation == 'relu'):\n",
    "            if self.istrain:                         # to prevent overflow we use leaky relu\n",
    "                return np.maximum(0, 0.01*x)\n",
    "            return np.maximum(0, x)\n",
    "    def sigmoid_derivative(self, x):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return x * (1 - x)\n",
    "        elif(self.activation == 'tanh'):\n",
    "            return 1 - x**2\n",
    "        elif(self.activation == 'relu'):\n",
    "            return (x > 0)\n",
    "    def mean_squared_error(self, predicted, actual):\n",
    "        return np.mean((predicted - actual)**2)\n",
    "    def forward_propagation(self, x):\n",
    "        self.layer_outputs = [x]\n",
    "        for i in range(len(self.neurons_layer) - 1):\n",
    "            z = np.dot(self.layer_outputs[i], self.weights[i]) + self.biases[i]\n",
    "            a = self.sigmoid(z) if i != len(self.neurons_layer) - 2 else z\n",
    "            self.layer_outputs.append(a)\n",
    "        return self.layer_outputs[-1]\n",
    "    def backward_propagation(self, X, y):\n",
    "        gradients = []\n",
    "        deltas = [2 * (self.layer_outputs[-1] - y)] #/ X.shape[0]]\n",
    "        for i in range(len(self.neurons_layer) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[-1], self.weights[i].T) * self.sigmoid_derivative(self.layer_outputs[i])\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "        num_samples = X.shape[0]\n",
    "        for i in range(len(self.hidden_layers), -1, -1):\n",
    "            grad_weights = np.dot(self.layer_outputs[i].T, deltas[i]) / num_samples\n",
    "            grad_biases = np.sum(deltas[i], axis=0, keepdims=True) / num_samples\n",
    "            gradients.append((grad_weights, grad_biases))\n",
    "        return gradients\n",
    "    def update(self, gradients):\n",
    "        gradients.reverse()\n",
    "        for i in range(len(self.hidden_layers) + 1):\n",
    "            grad_weights, grad_biases = gradients[i]\n",
    "            self.weights[i] -= self.learning_rate * grad_weights\n",
    "            self.biases[i] -= self.learning_rate * grad_biases\n",
    "    def train(self, X_train, y_train):\n",
    "        ep = self.epoch//10\n",
    "        self.istrain = 1\n",
    "        for epoch in range(ep):\n",
    "            for x, y in zip(X_train, y_train):\n",
    "                x = x.reshape(1, -1)\n",
    "                y = y.reshape(1, -1)\n",
    "                self.forward_propagation(x)\n",
    "                gradients = self.backward_propagation(x, y)\n",
    "                self.update(gradients)\n",
    "        self.istrain = 0\n",
    "    def trainbatch(self, x, y):\n",
    "        num_epochs = self.epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            gradients = []\n",
    "            self.forward_propagation(x)\n",
    "            gradients += self.backward_propagation(x, y)\n",
    "            gradients = [(np.mean(grad[0], axis=0), np.mean(grad[1], axis=0)) for grad in gradients]\n",
    "            self.update(gradients)\n",
    "    def trainminibatch(self, x, y):\n",
    "        num_epochs = self.epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            gradients = []\n",
    "            batch_size = 50\n",
    "            num_batches = x.shape[0] // batch_size\n",
    "            for i in range(num_batches):\n",
    "                x_batch = x[i * batch_size: (i + 1) * batch_size]\n",
    "                y_batch = y[i * batch_size: (i + 1) * batch_size]\n",
    "                self.forward_propagation(x_batch)\n",
    "                gradients += self.backward_propagation(x_batch, y_batch)\n",
    "                gradients = [(np.mean(grad[0], axis=0), np.mean(grad[1], axis=0)) for grad in gradients]\n",
    "                self.update(gradients)\n",
    "                gradients = []\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            x = x.reshape(1, -1)\n",
    "            predicted = self.forward_propagation(x)\n",
    "            predictions.append(predicted)\n",
    "        return np.vstack(predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier: #A3\n",
    "    def __init__(self, input_size, hidden_layers, learning_rate, activation, epoch):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.istrain = 0\n",
    "        self.epoch = epoch\n",
    "        self.encoder = OneHotEncoder()\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        y_temp = self.encoder.fit_transform(y_train.values.reshape(-1, 1)).toarray()\n",
    "        output_size = y_temp.shape[1]\n",
    "        # print(output_size)\n",
    "        self.output_size = output_size\n",
    "        self.neurons_layer = [self.input_size] + self.hidden_layers + [output_size]  \n",
    "        self.weights = [np.random.randn(self.neurons_layer[i], self.neurons_layer[i + 1]) for i in range(len(self.neurons_layer) - 1)]\n",
    "        self.biases = [np.zeros((1, neurons)) for neurons in self.neurons_layer[1:]]\n",
    "        self.trainminibatch(X_train, y_temp)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "    def sigmoid(self, x):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif(self.activation == 'tanh'):\n",
    "            exp_x = np.exp(2 * x)\n",
    "            tanh_x = (exp_x - 1) / (exp_x + 1)\n",
    "            return tanh_x\n",
    "        elif(self.activation == 'relu'):\n",
    "            if self.istrain:\n",
    "                return np.maximum(0, 0.01*x)\n",
    "            return np.maximum(0, x)\n",
    "    def sigmoid_derivative(self, x):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return x * (1 - x)\n",
    "        elif(self.activation == 'tanh'):\n",
    "            return 1 - x**2\n",
    "        elif(self.activation == 'relu'):\n",
    "            return (x > 0)\n",
    "    def forward_propagation(self, x):\n",
    "        self.layer_outputs = [x]\n",
    "        for i in range(len(self.neurons_layer) - 1):\n",
    "            z = np.dot(self.layer_outputs[i], self.weights[i]) + self.biases[i]\n",
    "            a = self.sigmoid(z) if i < len(self.neurons_layer) - 2 else self.softmax(z) \n",
    "            self.layer_outputs.append(a)\n",
    "        return self.layer_outputs[-1]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.forward_propagation(X)\n",
    "        return np.round(self.layer_outputs[-1], 5)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            x = x.reshape(1, -1)\n",
    "            predicted = self.forward_propagation(x)\n",
    "            predictions.append(predicted)\n",
    "        predicted = np.vstack(predictions)\n",
    "        y_temp = self.encoder.inverse_transform((predicted))\n",
    "        return y_temp\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        gradients = []\n",
    "        deltas = [self.layer_outputs[-1] - y]\n",
    "        for i in range(len(self.neurons_layer) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[-1], self.weights[i].T) * self.sigmoid_derivative(self.layer_outputs[i])\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "        num_samples = X.shape[0]\n",
    "        for i in range(len(self.hidden_layers), -1, -1):\n",
    "            grad_weights = np.dot(self.layer_outputs[i].T, deltas[i]) / num_samples\n",
    "            grad_biases = np.sum(deltas[i], axis=0, keepdims=True) / num_samples\n",
    "            gradients.append((grad_weights, grad_biases))\n",
    "        return gradients\n",
    "    def update(self, gradients):\n",
    "        gradients.reverse()\n",
    "        max = 0 \n",
    "        for i in range(len(self.hidden_layers) + 1):\n",
    "            grad_weights, grad_biases = gradients[i]\n",
    "            self.weights[i] -= self.learning_rate * grad_weights\n",
    "            self.biases[i] -= self.learning_rate * grad_biases\n",
    "            maxx = np.max(gradients[i][0])\n",
    "            if maxx > max:\n",
    "                max = maxx\n",
    "            maxx = np.max(gradients[i][1])\n",
    "            if maxx > max:\n",
    "                max = maxx\n",
    "    def train(self, X_train, y_train):\n",
    "        ep = self.epoch//10\n",
    "        self.istrain = 1\n",
    "        for epoch in range(ep):\n",
    "            for x, y in zip(X_train, y_train):\n",
    "                x = x.reshape(1, -1)\n",
    "                y = y.reshape(1, -1)\n",
    "                self.forward_propagation(x)\n",
    "                gradients = self.backward_propagation(x, y)\n",
    "                self.update(gradients)\n",
    "    def trainbatch(self, x, y):\n",
    "        for epoch in range(self.epoch):\n",
    "            gradients = []\n",
    "            self.forward_propagation(x)\n",
    "            gradients += self.backward_propagation(x, y)\n",
    "            gradients = [(np.mean(grad[0], axis=0), np.mean(grad[1], axis=0)) for grad in gradients]\n",
    "            self.update(gradients)\n",
    "    def trainminibatch(self, x, y):\n",
    "        for epoch in range(self.epoch):\n",
    "            gradients = []\n",
    "            batch_size = 50\n",
    "            num_batches = x.shape[0] // batch_size\n",
    "            for i in range(num_batches):\n",
    "                x_batch = x[i * batch_size: (i + 1) * batch_size]\n",
    "                y_batch = y[i * batch_size: (i + 1) * batch_size]\n",
    "                self.forward_propagation(x_batch)\n",
    "                gradients += self.backward_propagation(x_batch, y_batch)\n",
    "                gradients = [(np.mean(grad[0], axis=0), np.mean(grad[1], axis=0)) for grad in gradients]\n",
    "                self.update(gradients)\n",
    "                gradients = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier2: #A3\n",
    "    def __init__(self, input_size, hidden_layers, learning_rate, activation, epoch):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.istrain = 0\n",
    "        self.epoch = epoch\n",
    "        self.encoder = OneHotEncoder()\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        y_temp = self.encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "        output_size = y_temp.shape[1]\n",
    "        # print(output_size)\n",
    "        self.output_size = output_size\n",
    "        self.neurons_layer = [self.input_size] + self.hidden_layers + [output_size]  \n",
    "        self.weights = [np.random.randn(self.neurons_layer[i], self.neurons_layer[i + 1]) for i in range(len(self.neurons_layer) - 1)]\n",
    "        self.biases = [np.zeros((1, neurons)) for neurons in self.neurons_layer[1:]]\n",
    "        self.trainminibatch(X_train, y_temp)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "    def sigmoid(self, x):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif(self.activation == 'tanh'):\n",
    "            exp_x = np.exp(2 * x)\n",
    "            tanh_x = (exp_x - 1) / (exp_x + 1)\n",
    "            return tanh_x\n",
    "        elif(self.activation == 'relu'):\n",
    "            if self.istrain:\n",
    "                return np.maximum(0, 0.01*x)\n",
    "            return np.maximum(0, x)\n",
    "    def sigmoid_derivative(self, x):\n",
    "        if(self.activation == 'sigmoid'):\n",
    "            return x * (1 - x)\n",
    "        elif(self.activation == 'tanh'):\n",
    "            return 1 - x**2\n",
    "        elif(self.activation == 'relu'):\n",
    "            return (x > 0)\n",
    "    def forward_propagation(self, x):\n",
    "        self.layer_outputs = [x]\n",
    "        for i in range(len(self.neurons_layer) - 1):\n",
    "            z = np.dot(self.layer_outputs[i], self.weights[i]) + self.biases[i]\n",
    "            a = self.sigmoid(z) if i < len(self.neurons_layer) - 2 else self.softmax(z) \n",
    "            self.layer_outputs.append(a)\n",
    "        return self.layer_outputs[-1]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.forward_propagation(X)\n",
    "        return np.round(self.layer_outputs[-1], 5)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            x = x.reshape(1, -1)\n",
    "            predicted = self.forward_propagation(x)\n",
    "            predictions.append(predicted)\n",
    "        predicted = np.vstack(predictions)\n",
    "        y_temp = self.encoder.inverse_transform((predicted))\n",
    "        return y_temp\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        gradients = []\n",
    "        deltas = [self.layer_outputs[-1] - y]\n",
    "        for i in range(len(self.neurons_layer) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[-1], self.weights[i].T) * self.sigmoid_derivative(self.layer_outputs[i])\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "        num_samples = X.shape[0]\n",
    "        for i in range(len(self.hidden_layers), -1, -1):\n",
    "            grad_weights = np.dot(self.layer_outputs[i].T, deltas[i]) / num_samples\n",
    "            grad_biases = np.sum(deltas[i], axis=0, keepdims=True) / num_samples\n",
    "            gradients.append((grad_weights, grad_biases))\n",
    "        return gradients\n",
    "    def update(self, gradients):\n",
    "        gradients.reverse()\n",
    "        max = 0 \n",
    "        for i in range(len(self.hidden_layers) + 1):\n",
    "            grad_weights, grad_biases = gradients[i]\n",
    "            self.weights[i] -= self.learning_rate * grad_weights\n",
    "            self.biases[i] -= self.learning_rate * grad_biases\n",
    "            maxx = np.max(gradients[i][0])\n",
    "            if maxx > max:\n",
    "                max = maxx\n",
    "            maxx = np.max(gradients[i][1])\n",
    "            if maxx > max:\n",
    "                max = maxx\n",
    "    def train(self, X_train, y_train):\n",
    "        ep = self.epoch//10\n",
    "        self.istrain = 1\n",
    "        for epoch in range(ep):\n",
    "            for x, y in zip(X_train, y_train):\n",
    "                x = x.reshape(1, -1)\n",
    "                y = y.reshape(1, -1)\n",
    "                self.forward_propagation(x)\n",
    "                gradients = self.backward_propagation(x, y)\n",
    "                self.update(gradients)\n",
    "    def trainbatch(self, x, y):\n",
    "        for epoch in range(self.epoch):\n",
    "            gradients = []\n",
    "            self.forward_propagation(x)\n",
    "            gradients += self.backward_propagation(x, y)\n",
    "            gradients = [(np.mean(grad[0], axis=0), np.mean(grad[1], axis=0)) for grad in gradients]\n",
    "            self.update(gradients)\n",
    "    def trainminibatch(self, x, y):\n",
    "        for epoch in range(self.epoch):\n",
    "            gradients = []\n",
    "            batch_size = 50\n",
    "            num_batches = x.shape[0] // batch_size\n",
    "            for i in range(num_batches):\n",
    "                x_batch = x[i * batch_size: (i + 1) * batch_size]\n",
    "                y_batch = y[i * batch_size: (i + 1) * batch_size]\n",
    "                self.forward_propagation(x_batch)\n",
    "                gradients += self.backward_propagation(x_batch, y_batch)\n",
    "                gradients = [(np.mean(grad[0], axis=0), np.mean(grad[1], axis=0)) for grad in gradients]\n",
    "                self.update(gradients)\n",
    "                gradients = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
